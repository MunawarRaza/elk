ELK Stack
Beats: Data Collections
Logstash: Data aggregation and processing
Elastic Search: indexing and storage
kibana: Analysis and visualization


Elasticsearch 8.14.x, java-17
Kibana 8.12.0
Logstash 8.14.x, java-17
kibana: 8.12.x

elasticsearch ports:
9200 for http trafic
9300 for 

kibana port: 5601
logstash port: 9600-9700 and 5044 (to get connection of filebeat)

sudo filebeat setup --index-management -E output.logstash.enabled=false -E 'output.elasticsearch.hosts=["10.130.21.90:9200"]'

### install elasticsearch ###

wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo gpg --dearmor -o /usr/share/keyrings/elasticsearch-keyring.gpg
sudo apt-get install apt-transport-https
echo "deb [signed-by=/usr/share/keyrings/elasticsearch-keyring.gpg] https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee /etc/apt/sources.list.d/elastic-8.x.list
sudo apt-get update && sudo apt-get install elasticsearch

change configuration files before installing logstash
vim /etc/elasticsearch/elasticsearch.yml
xpack.security.enabled: true to xpack.security.enabled: false
xpack.security.enrollment.enabled: true to xpack.security.enrollment.enabled: false
xpack.security.transport.ssl:
  enabled: true to false
xpack.security.http.ssl:
  enabled: true to false
systemctl restart elasticsearch
curl -X GET "localhost:9200"

### install logstash ###
sudo apt-get install logstash

before starting logstash, create dummy pipeline otherwise you will get error
vim /etc/logstash/conf.d/first.conf
input {
    beats {
        port => "5044"
    }
}
output {
    elasticsearch {
      hosts => ["10.100.79.93:9200"]
      index => "jboss-%{+YYYY.MM.dd}"
    }
    stdout { codec => rubydebug }
}

sudo systemctl restart logstash.service


### Install Filebeat ###

wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
sudo apt-get install apt-transport-https
echo "deb https://artifacts.elastic.co/packages/8.x/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-8.x.list
sudo apt-get update && sudo apt-get install filebeat

https://www.elastic.co/guide/en/beats/filebeat/current/setup-repositories.html


Cluster: collections of one or more nodes
Node: single server for storing data. It is called uuid 
Shards: distribute data accorss multiple nodes for better performance
Replicas: These are the replicas of shards. e.g I have one index and multiple types in that index. My data will be distributed on multiple node meaning, one type would be on a different server and second type on different server. These would be replicated on other server so that if one shard have issues, my data is safe
Index: Database name/ schema is called index
Types: 1 table in RDBMS is called one type.
Fields: called column in relational DB
Documents with properties: One record/row is called one document Data in JSON form. one row in RDBMS is equal to one document
e.g {
	"ID": 1
	"Name": "Munawra",
	"Positon": "DevOps Engineer",
	"Age": 31
	}
Above is a single document or single record
Mapping: Formate of data values ( user_id, first_name, last_name, age ) with data types

RDBMS --> Databases --> Tables --> Columns/Rows
Elasticsearch --> Index --> Types --> Documents with properties

RDBMS --> Database --> Tables --> Columns/Rows
Elasticsearch --> Index(tables) --> documents (rows)

Analysis: Converting text into Token or terms
e.g
sentence: "A quick brown fox jumped over the lazy dog"
Tokens: [quick, brown, fox, jumped, over, lazy, dog]
Analysis performed by:

document = json + metadat
	all keys are store with _keys

document
e.g {

	"ID": 1
	"Name": "Munawra",
	"Positon": "DevOps Engineer",
	"Age": 31
	}


Filebeat:
	It collects the data and send to elasticsearch or to logstash.
	3 ways to collect the data
		- Data collection modules — simplify the collection, parsing, and visualization of common log formats
		- ECS loggers — structure and format application logs into ECS-compatible JSON
		- Manual Filebeat configuration
	list the available modules
		filebeat modules list
	enable the module
		filebeat modules enable nginx
How filebeat work
	There are two main components: input and harvesters. These components work togather to tail files and send event data to the output we specify in filebeat configurations.
What is a harvester?
	A harvester is responsible for reading the content of a single file. The harvester reads each file, line by line, and sends the content to the output. One harvester is started for each file. The harvester is responsible for opening and closing the file, which means that the file descriptor remains open while the harvester is running. If a file is removed or renamed while it’s being harvested, Filebeat continues to read the file. This has the side effect that the space on your disk is reserved until the harvester closes. By default, Filebeat keeps the file open until close_inactive is reached
What is an input?
	An input is responsible for managing the harvesters and finding all sources to read from.If the input type is log, the input finds all files on the drive that match the defined glob paths and starts a harvester for each file. Each input runs in its own Go routine.


installation
configuration
directory structure


# Check elastic search
curl -X GET "10.130.21.90:9200"
# List the indices
curl -XGET http://10.130.21.90:9200/_cat/indices?

# Create new indices
curl -XPUT '10.130.21.90:9200/twitter?pretty' -H 'Content-Type: application/json' -d'{"settings" : {"index" : {"number_of_shards" : 3, "number_of_replicas" : 0 }}}'
# Delete indices
curl -X DELETE "http://10.130.21.90:9200/twitter"
 
# Explore the indices
curl -X GET "10.130.21.90:9200/twitter/_search?pretty"



curl -X PUT "localhost:9200/_security/user/admin" -H 'Content-Type: application/json' -d '{"username": "admin1", "password": "password", "roles": ["superuser"]}'



sudo /usr/share/elasticsearch/bin/elasticsearch-create-enrollment-token -s node

eyJ2ZXIiOiI4LjE0LjAiLCJhZHIiOlsiMTAuMTMwLjIxLjkwOjkyMDAiXSwiZmdyIjoiNDkxYzQ5YmM3YmJkMGU2NTJlYzMzYzNkMGQ3ZGJjZDdlNjAxZTQ0MzA2MzcwMzFmZmFiNTMxMzNiNjZmYjYzMiIsImtleSI6InJIUXdNNVVCMFluQzFXbmY3eWkzOjhQX1pKdDZwUnBLblZCU01uSnRIUVEifQ==

sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u kibana_system

sudo /usr/share/elasticsearch/bin/elasticsearch-reset-password -u elastic
	nlWFd5n8AFf1Z8QNpfB8

sudo /usr/share/elasticsearch/bin/elasticsearch-service-tokens create elastic/kibana default
# SERVICE_TOKEN elastic/kibana/default = AAEAAWVsYXN0aWMva2liYW5hL2RlZmF1bHQ6QlFZT3hsQzRTQTZNNFJuanJlZEpEZw


curl -H "Authorization: Bearer AAEAAWVsYXN0aWMva2liYW5hL2RlZmF1bHQ6QlFZT3hsQzRTQTZNNFJuanJlZEpEZw" http://localhost:9200/_security/_authenticate



sudo /usr/share/elasticsearch/bin/elasticsearch-users useradd munawar -p 5hL2RlZmF1 -r superuser



curl -u elastic -X PUT "http://localhost:9200/_security/user/munawar" -H 'Content-Type: application/json' -d'
{
  "password": "5hL2RlZmF1",
  "roles": ["superuser"]
}'



user: elastic
pass: xT8oOfF4VECeU-zEp4Dx

user: kibana_system
password: GKhHsNMldXi0_SBadDKg


curl -u elastic -X POST "http://10.130.21.90:9200/_security/user/logstash_system/_password" -H "Content-Type: application/json" -d '{
  "password": "Acdadf!!123adsf"
}'

########## How to enable User management Module in ELK

1. Configure Elasticsearch
	Enable Security:
		Open the elasticsearch.yml file
		Add or modify the setting: xpack.security.enabled: true
		restart the elasticsearch service
	Set Passwords:
		Command to set passwords for the following built-in users: elastic, kibana_system
			bin/elasticsearch-setup-passwords interactive -u elastic
			bin/elasticsearch-setup-passwords interactive -u kibana_system
	OR Reset Passwords:
		Commands to reset the password of built-in users
			bin/elasticsearch-setup-passwords interactive -u elastic
			bin/elasticsearch-setup-passwords interactive -u kibana_system

	Note: Save the passwords securely as you'll need them later.
2. Configure Kibana:
	Specify Kibana User:
		Open the kibana.yml file
		Add or modify the setting: elasticsearch.username: "kibana_system"
		Add or modify the setting: elasticsearch.password: "GKhHsNMldXi0_SBadDKg"
		Restart the kibana service
	Access the kibana in browser
		Username: elastic
		Password: Generated by bin/elasticsearch-setup-passwords interactive -u elastic

	Note: If service not started or found issue then do the following steps
	Configure Keystore:
		Create the Kibana keystore if it doesn't exist: bin/kibana-keystore create
		Add the password for the kibana_system user to the keystore: bin/kibana-keystore add elasticsearch.password
		You'll be prompted to enter the password you set for kibana_system

Step 3: Set Up Role-Based Access Control (RBAC)
In Kibana, navigate to Management > Stack Management > Roles to define roles with specific permissions.
Go to Users to assign roles to users.



